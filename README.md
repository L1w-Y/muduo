# 万字剖析muduo高性能网络库设计细节
## 📚 目录（Index）
- [一、前置知识](#一前置知识)
  - [1.1 阻塞/非阻塞 与 同步/异步](#11-阻塞非阻塞-与-同步异步)
      - [系统调用层面](#1-io系统调用层面)
      - [应用层面](#2-应用层面)
  - [1.2 五种IO模型](#12-五种IO模型)
      - [阻塞IO](#🟢阻塞IO)
      - [非阻塞IO](#🟢非阻塞IO)
      - [IO多路复用](#🟢IO-多路复用)
        - [Select](#🟡select)
        - [Poll](#🟡poll)
        - [epoll](#🟡epoll)
      - [信号驱动](#🟢信号驱动)
      - [异步](#🟢异步)

- [Channel类](#channel类)

- [Poller类](#poller类)

- [EventLoop](#eventloop)
  - [三个类之间的协作](#三个类之间的协作)
  - [线程间通信](#线程间通信)
  - [共享资源？每个thread一个loop](#共享资源每个thread-一个loop)
  
## 一、前置知识

在进入正题之前，先聊一聊网络编程中一些重要的基本概念。

### 1.1 阻塞/非阻塞 与 同步/异步

`阻塞/非阻塞` 和 `同步/异步` 是两个不同维度的概念，我们分别在系统调用和应用层面上进行讨论：

---

### 1. IO系统调用层面

可以说正是因为底层 I/O 系统调用的行为（如`read`, `write`）存在这两种维度，才构成了上层进程通信模型的不同概念

#### - 🟢**Blocking vs Non-blocking**

描述的是**单个系统调用是否会使进程挂起**，以下以 `read()` 为例：

- **Blocking I/O**：  
  当调用 `read()` 后，如果内核缓冲区没有数据，内核会让该进程从「运行态」进入「睡眠态」。CPU 会调度执行其他进程。直到数据准备好并被复制到用户空间缓冲区后，内核才会唤醒该进程，`read()` 调用才会返回。这个过程中，进程是被阻塞的。

- **Non-blocking I/O**：  
  通过设置文件描述符的 `O_NONBLOCK` 标志实现。当用户进程调用 `read()` 时，如果内核缓冲区没有数据，系统调用会立即返回错误码。进程不会被阻塞，可以继续执行其他代码。通常用户会通过轮询方式在稍后再次尝试调用 `read()`。

#### -🟢**Synchronous vs Asynchronous**

描述的是**整个 I/O 操作（从发起请求到数据最终可用）是由谁来完成的**，以及结果是如何被通知给进程的。

- **Synchronous I/O**：  
  在同步 I/O 中，用户进程是发起 I/O 操作的主体，并且需要主动等待或查询 I/O 操作的结果。
  
  **🌟🌟在处理 IO 的时候，阻塞和非阻塞都是同步 IO。只有使用了特殊的 API 才是异步 IO🌟🌟** 
  - 阻塞 I/O 是同步的，因为进程通过阻塞来“等待”结果。
  - 非阻塞 I/O 也是同步的，因为进程需要通过反复轮询（polling）来“查询”操作是否完成。
  - I/O 多路复用（如 `select`, `epoll`）同样是同步的。虽然它们可以同时监听多个文件描述符，看起来像“异步”，但本质上 `select/epoll` 调用会阻塞进程以“等待”事件就绪。而且当事件返回后，数据还在内核缓冲区，用户进程仍需调用 `read()` 才能完成数据读取。因此整个 I/O 操作仍然是由用户主动完成的。

- **Asynchronous I/O**：  
  异步 I/O（如 `aio_read()`）中，用户进程发起请求后立即返回，可以做其他事。  
  内核会独立完成两个阶段：

  1. 等待数据准备好；
  2. 将数据从内核缓冲区拷贝到用户指定缓冲区。

  当这两个阶段完成后，内核通过 **信号** 或 **回调函数** 的方式通知用户进程：**数据已准备完毕且拷贝完成**。
  
  也就是说在异步io中，数据的读写是由内核帮助用户完成的。

---

### 2. 应用层面

在应用层面，这些概念更多地关注程序如何组织逻辑、等待结果、以及响应 I/O 的方式。

#### - 🟢**Blocking vs Non-blocking**

当你调用某个函数（通常是 I/O 相关，比如读取 socket、发请求等）时，当前线程是否被挂起等待结果返回。

*   **Blocking I/O**：
    当应用程序执行一个操作（比如调用一个函数去获取远程 API 数据，或查询数据库）时，如果该操作需要一些时间来完成，应用程序的当前执行线程会等待该操作返回结果后才能继续执行后续代码。
    如果某个任务耗时较长，整个应用（或至少是该线程）会显得 "卡顿" 或无响应。

    **例子**：一个传统的单线程 Web 服务器，当它为一个请求查询数据库时，它必须等待数据库返回结果，在此期间无法处理其他新的请求。

*   **Non-blocking I/O**：
    当应用程序执行一个操作时，该操作会立即返回，而不会等待其真正完成。应用程序可以继续执行其他任务。
    应用程序不会因为等待某个操作而被挂起，通常需要一种机制来稍后获取操作的结果，比如通过轮询检查状态，或者注册一个回调函数。

#### - 🟢**Synchronous vs Asynchronous**

在应用层面的同步和异步更多是描述 **整个操作或者业务逻辑的完成过程，是由调用方主动去完成，还是系统或框架异步地帮你完成后，通知你结果**。

*   **Synchronous I/O**：
    整个流程是顺序的，即使底层使用了非阻塞 I/O，如果应用逻辑设计成 "发起 -> 等待结果 -> 处理结果" 的串行模式，那么从应用行为上看它仍然是同步的。

*   **Asynchronous I/O**：
    在应用层面的一个处理逻辑，比如 A 向 B 请求调用一个网络 I/O 接口时（或者调用某个业务逻辑 API 接口时），向 B 传入请求的事件以及事件发生时通知的方式，A 就可以处理其它逻辑了。
    当 B 监听到事件处理完成后，会用事先约定好的通知方式，通知 A 处理结果。

    基于 reactor 的高性能网络框架多是采用这样的非阻塞 + 异步的设计模式，开发者注册相应事件和对应事件发生的回调函数，在监听到相应事件发生时，网络框架主动调用回调函数。
    虽然在网络库内部的事件监听和 I/O 读写操作时，采用的系统接口比如 `epoll_wait` 和 `recv()` 等等仍是同步的过程，但在上层应用层面的事件注册和处理逻辑是在异步执行。

---

### 1.2 五种IO模型
---
#### 🟢**阻塞IO**
在所示的这样一个传统的socket流程中，服务器端代码阻塞在了accept()和read()两个函数上
![阻塞IO](res/阻塞io流程.gif)

再看read()函数阻塞的内部流程,其实发生了两次阻塞

![read函数](res/read流程.png)
![read函数](res/read函数内部.gif)

这种模式有相当大的弊端，如果对端一直不发数据，那么服务端线程将会一直阻塞在 read 函数上死等，无法进行任何其他的操作。

---

#### 🟢**非阻塞IO**
非阻塞模式是通过操作系统提供的api，将文件描述符设置为非阻塞状态，比如：
```cpp
fcntl(connfd, F_SETFL, O_NONBLOCK);
int n = read(connfd, buffer) != SUCCESS);
```
这种状态下的read函数在调用够后会直接返回，不管数据是否准备号，用户通过返回值的不同类型来自行判断，通常用户层面采用反复调用的方式来配合调用，如图：

![read函数](res/非阻塞read.png)
![read函数](res/非阻塞read.gif)

可以看到这种系统调用不会挂住程序，避免线程阻塞。

但是这里要注意，在数据还未到达网卡，或者到达网卡但还没有拷贝到内核缓冲区之前，这个阶段是非阻塞的。当数据已到达内核缓冲区，**此时调用 read 函数仍然是阻塞的**，需要等待数据从内核缓冲区拷贝到用户缓冲区，才能返回。

非阻塞 I/O 本身不是缺点，但是单纯的非阻塞 I/O 上需要搭配反复轮询去资源是否就绪，就会造成CPU空转，造成资源浪费，可以认为是非阻塞 I/O 的一种使用方式上的缺陷。

---
#### 🟢**IO 多路复用**
前面两种情况都是一次监听一个文件描述符，而IO多路复用就是：

**🌟同时监听多个文件描述符，并在有一个或多个就绪（可读/可写/异常）时，通知用户进行处理的机制🌟**
  #### - 🟡**select**
  select 是操作系统提供的系统调用函数，通过它，我们可以把一个文件描述符的数组发给操作系统， 让操作系统去遍历，确定哪个文件描述符可以读写， 然后告诉我们去处理：

  ![select](res/select_1.gif)

  通常采用select设计如下：
  ```cpp
  //一个线程不断接受客户端连接，并把 socket 文件描述符放到一个 list 里
  while(1) {
  connfd = accept(listenfd);
  fcntl(connfd, F_SETFL, O_NONBLOCK);
  fdlist.add(connfd);
  }

  //然后另起一个线程调用 select，将这批文件描述符 list 交给操作系统去遍历
  while(1) {
  /*
  把一堆文件描述符 list 传给 select 函数
  有已就绪的文件描述符就返回，nready 表示有多少个就绪的
  */
  nready = select(list);
  ...
  ...
  }
  ```
  但是在select返回后，操作系统会将准备就绪的文件描述符做上标识，用户其实还需要自己对文件描述符数组进行遍历，来找到有事件发生的fd。

  select有几个细节：
  1. select 调用需要传入 fd 数组，需要拷贝一份到内核，高并发场景下这样的拷贝消耗的资源是惊人的
  2. select 在内核层仍然是通过遍历的方式检查文件描述符的就绪状态，是个同步过程，只不过无系统调用切换上下文的开销，当连接数量很多时，这个轮询的成本变成 O(n)
   
  3. select 仅仅返回可读文件描述符的个数，具体哪个可读还是要用户自己遍历。（可优化为只返回给用户就绪的文件描述符，无需用户做无效的遍历）

  #### - 🟡**poll**
  poll对select进行了优化，除去了 select 只能监听 1024 个文件描述符的限制

  #### - 🟡**epoll**

  最后的epoll针对上述select中提到的几个细节进行了优化：
  1. 文件描述符集合采用红黑树的结构保存在内核中，无需用户每次都重新传入，只需告诉内核修改的部分即可
  2. 内核不再通过轮询的方式找到就绪的文件描述符，而是通过异步 IO 事件唤醒
  3. 内核仅会将有 IO 事件的文件描述符返回给用户，用户也无需遍历整个文件描述符集合 
  
  **🌟`epoll`的机制：基于`事件驱动+回调`🌟**
  通常epoll的三步使用流程是：
  1. 注册事件，只需要注册一次，通过epoll_ctl()向存在内核的epoll红黑树上挂fd和对应的回调函数指针
  ```cpp
  //告诉内核：“我关心哪个 fd 上的哪个事件，比如 EPOLLIN（可读）。”
  epoll_ctl(epfd, EPOLL_CTL_ADD, fd, &event);
  ```
  2. 等待事件就绪（阻塞状态） 
  
  内核会在你注册的 fd 上监听事件,一旦有事件发生，会立即“唤醒”你。
  ```cpp
  int n = epoll_wait(epfd, events, MAX_EVENTS, timeout);
  ```
  3. 只处理就绪 fd

  所以epoll不再对fd进行轮询

  - epoll 内核里使用了红黑树 + 就绪链表来管理监听 fd；
  - 当你调用 epoll_ctl 注册 fd 时，fd 被挂到内核维护的数据结构中；
  - 当某个 fd 状态变化（如收到数据），内核通过中断或回调直接把这个 fd 放入就绪列表；
  - epoll_wait 在睡眠中被内核事件驱动唤醒，只返回已经就绪的 fd。

  ✅ 你监听了 10000 个连接，如果只 3 个就绪，epoll_wait 只返回那 3 个，无需你自己轮询。

  过程如下：

  ![select](res/epoll.gif)

---


所以在io多路复用中，整体的流程是这样的：

![select](res/io多路复用.png)

---

####  🟢**信号驱动**

信号驱动的核心思想是：

**利用内核信号机制，在 I/O 就绪时发送一个信号通知进程，然后再由进程读取数据。**
换句话说就是
* 程序不需要一直轮询检查 fd 是否可读，
* 而是内核主动 发一个 SIGIO 信号，通知你：“数据来了”，
* 程序在信号处理函数中再去调用 read() 等函数进行读写操作。

![select](res/信号驱动.png)

内核在第一个阶段是异步的，第二个阶段是同步的；与非阻塞IO的区别在于它提供了消息通知机制，不需要用户进程不断的轮询检查，减少了系统API的调用次数，提高了效率

---

####  🟢**异步**
应用程序发起 I/O 请求后，立即返回，不需要等待内核完成操作；当内核完成 I/O 操作后，主动通知应用程序（比如通过回调或信号），并附带读/写的结果。

 与非阻塞 I/O 不同：非阻塞 I/O 仍然要自己调用 read()/write()，而 AIO 是由内核完成操作并返回结果，真正的“交给内核全权处理”

![select](res/异步.png)

像nginx，Netty底层采用的就是操作系统提供的异步io接口实现的网络库

---

## Channel类
       
设计思想 
        
作用     
        
代码细节 
        
## Poller类
作用    

设计方法，静态工厂

抽象接口

派生类构造

公共的DefaultPoller.cc

## eventloop

三个类之间的协作

线程间通信

共享资源？每个thread 一个loop

## Thread EventLoopThread EventLoopThreadPool

EventLoopThread的逻辑是
构造时将thread_(std::bind(&EventLoopThread::threadFunc,this),name)绑定，作为thread类的func_回调在调用startLoop时，调用thread_.start()，创建子线程用智能指针管理，然后子线程内部又调用func_（），也就是EventLoopThread的threadFunc()方法，在这个方法内创建了一个loop,调用外部传给的EventLoopThread的callback_，然后在这个threadFunc()回调中执行loop.loop();
两个问题
1.设计成这样复杂的回调逻辑是为什么
2.thread_(std::bind(&EventLoopThread::threadFunc,this),name)2.thread_(std::bind(&EventLoopThread::threadFunc,this),name)将类内部成员方法传给thread_，但是构造时是move,这样内成员方法不就是脱离对象实例在调用吗